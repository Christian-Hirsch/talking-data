{
 "metadata": {
  "name": "",
  "signature": "sha256:32fdf723b4b88c0c55e55770bc5b361ccc3382e6c271eabe2c49919e52920b97"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "TalkingData -- Feature Engineering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the second round of feature engineering, we construct features from the 'events' data. This will result in features involving the location of the event. After the discussions at Kaggle it seems that the location data can be omitted."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math\n",
      "import pandas as pd\n",
      "import pickle\n",
      "import time\n",
      "\n",
      "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
      "\n",
      "#path to data and features\n",
      "DATA_PATH = \"../../../input/\"\n",
      "FEATURE_PATH = \"../../../features/\"\n",
      "\n",
      "#hour marking the begin of a day\n",
      "DAWN = 5\n",
      "\n",
      "#Minimum number of events starting from which no smoothing is done\n",
      "NMIN = 25"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we load the train-test data and merge with the events data. We perform an inner join so as to keep only those device ids for which some events are available."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train = pd.read_csv('{0}gender_age_train.csv'.format(DATA_PATH))['device_id']\n",
      "test = pd.read_csv('{0}gender_age_test.csv'.format(DATA_PATH))['device_id']\n",
      "events = pd.read_csv('{0}events.csv'.format(DATA_PATH))\n",
      "\n",
      "train_test = pd.concat([train, test])\n",
      "train_test_events = pd.merge(train_test.to_frame(), events, 'inner', on = 'device_id').drop_duplicates().set_index('device_id')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Event count (1 feature)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To begin with, we simply count the number of events per device id."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "event_cnt = train_test_events.groupby(level = 0, sort = False)['event_id'].nunique()\n",
      "cnt_name = ['event_cnt']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a pleasant side effect, we obtain device-ids that have at least some events associated with them. We extract and pickle them for further use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "event_cnt_train = pd.merge(train.to_frame(), event_cnt.to_frame(), 'inner', \n",
      "                       left_on = 'device_id', right_index = True).set_index('device_id')\n",
      "event_cnt_test = pd.merge(test.to_frame(), event_cnt.to_frame(), 'inner',\n",
      "                      left_on = 'device_id', right_index = True).set_index('device_id')\n",
      "\n",
      "pickle.dump(event_cnt_train.index, open('{0}train_event_ids'.format(DATA_PATH), 'wb'))\n",
      "pickle.dump(event_cnt_test.index, open('{0}test_event_ids'.format(DATA_PATH), 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Time features (1 + 16 features)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the timestamp column, we first extract weekday and hour, and quantize the predictors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dates = pd.to_datetime(train_test_events['timestamp'])\n",
      "\n",
      "start = time.clock()\n",
      "hours = dates.dt.hour\n",
      "binned_hours = pd.cut((hours - DAWN ) % 24 + DAWN, [5, 7 , 22, 28], \n",
      "                      labels = ['morning', 'day', 'night']).to_frame().rename(columns = {'timestamp': 'binned_hour'})\n",
      "weekdays = pd.cut(dates.dt.weekday, [0, 3, 4, 5, 6], \n",
      "                  labels = ['weekday', 'friday', 'saturday', 'sunday'], \n",
      "                  include_lowest = True).to_frame().rename(columns = {'timestamp': 'weekday'})\n",
      "weekdays_binned_hours = pd.concat([weekdays, binned_hours], axis = 1).apply(lambda row: '{0},{1}'.format(row[0],row[1]), axis = 1)\n",
      "weekdays_binned_hours.name = 'timestamp'\n",
      "print(time.clock() - start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "463.1283760000001\n"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a first rough feature, we consider the mean of the shifted hours corresponding to a device-id. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean_hour = event_cnt.to_frame().merge(((hours - DAWN ) % 24 + DAWN).groupby(level = 0).mean().to_frame(),\n",
      "                           'left',left_index = True, right_index = True).drop('event_id', axis = 1).fillna(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 122
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each device, we make a statistic of when an event was triggered. To make the features comparable, we compute the log-likelihoods. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_logliks_pivot(data):\n",
      "    hist_raw = data.reset_index().pivot_table(index = 'device_id', columns = data.name, aggfunc = len, fill_value = 0)\n",
      "    hist = event_cnt.to_frame().merge(hist_raw,'left',left_index = True, right_index = True).drop('event_id', axis = 1)\n",
      "\n",
      "    prior = data.value_counts(normalize = True).sort_index().values\n",
      "    return hist.apply(lambda row: compute_logliks(row, prior), axis = 1).fillna(0)\n",
      "\n",
      "def compute_logliks(row, prior):\n",
      "    #add 1 to avoid taking logarithm of 0\n",
      "    row = row + 1\n",
      "    \n",
      "    #smooth out the probabilities by using the prior\n",
      "    row_sum = row.sum()\n",
      "    weight = min(row_sum - len(row), NMIN)/NMIN\n",
      "    row = (1 - weight) * prior + weight * (row / row_sum)\n",
      "        \n",
      "    #compute the log ratios of class probabilities and the popularity of the feature \n",
      "    row = row.apply(lambda y: math.log(y) - math.log(1.0 / len(row)))\n",
      "    \n",
      "    return row    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we compute these statistics for hours and for the binned days."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start = time.clock()\n",
      "days_loglik = compute_logliks_pivot(weekdays_binned_hours)\n",
      "print(time.clock() - start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "338.1495399999999\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "time_features = np.hstack([ days_loglik, mean_hour]) \n",
      "time_names = np.hstack([days_loglik.columns, 'mean_hour']) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Concluding feature union"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we collect the event features and persist them to disk."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "event_features = np.hstack([event_cnt.to_frame(), time_features])\n",
      "event_features_names = np.hstack([cnt_name, time_names])\n",
      "\n",
      "pickle.dump(event_features, open('{0}event_features_noloc.p'.format(FEATURE_PATH), 'wb'))\n",
      "pickle.dump(event_features_names, open('{0}event_features_noloc_names.p'.format(FEATURE_PATH), 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}